{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLOWFAST\n",
    "https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/ (accessed 22-03-2023)\n",
    "\n",
    "By FAIR PyTorchVideo\n",
    "\n",
    "SlowFast networks pretrained on the Kinetics 400 dataset\n",
    "\n",
    "| arch     | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
    "|----------|-------|----------------------------|-------|-------|-----------|------------|\n",
    "| SlowFast | R50   | 8x8                        | 76.94 | 92.69 | 65.71     | 34.57      |\n",
    "| SlowFast | R101  | 8x8                        | 77.90 | 93.27 | 127.20    | 62.83      |\n",
    "\n",
    "Best performing of the available models in PyTorch Hub\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] Christoph Feichtenhofer et al, “SlowFast Networks for Video Recognition” https://arxiv.org/pdf/1812.03982.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append('..')\n",
    "# from utils import \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the `slowfast_r50` model \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo # type: ignore\n",
    "from pytorchvideo.transforms import ( # type: ignore\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Set the model to eval mode and move to desired device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to GPU or CPU\n",
    "device = \"cuda\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "slowfast_alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "clip_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/homes/lm004/commercials/annotated_commercials/_3GbX2_IaI8_trimmed.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path):\n",
    "    # Initialize an EncodedVideo helper class and load the video\n",
    "    video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "\n",
    "    start_sec = 4 # initial offset to avoid eventual black frames\n",
    "\n",
    "    # take a maximum of 4 clips if the video is long enough\n",
    "    num_clips = min(4, (video.duration.__float__() - start_sec) // clip_duration)\n",
    "\n",
    "    inputs_list = []\n",
    "\n",
    "    for _ in range(num_clips):\n",
    "        end_sec = start_sec + clip_duration\n",
    "\n",
    "        # Load the desired clip\n",
    "        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "\n",
    "        # Move the inputs to the desired device\n",
    "        inputs = video_data[\"video\"]\n",
    "        inputs = [i.to(device)[None, ...] for i in inputs]\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "        # start from the end of the previous clip\n",
    "        start_sec = end_sec\n",
    "\n",
    "    return inputs_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REGISTER HOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach() # type: ignore\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks.get_submodule(\"5\").pool.get_submodule(\"1\").register_forward_hook(get_features('layer5_fast'))\n",
    "model.blocks.get_submodule(\"5\").pool.get_submodule(\"0\").register_forward_hook(get_features('layer5_slow'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_slow_list = []\n",
    "features_fast_list = []\n",
    "\n",
    "for inputs in preprocess_video(video_path):\n",
    "    # placeholder for features\n",
    "    features = {}\n",
    "\n",
    "    # Pass the input clip through the model\n",
    "    preds = model(inputs)\n",
    "\n",
    "    # Get the features from the last layer\n",
    "    # NB: compute the mean of the features across the last dimensions\n",
    "    features_fast_list.append(features['layer5_fast'].cpu().numpy().mean(axis=(2,3,4)))\n",
    "    features_slow_list.append(features['layer5_slow'].cpu().numpy().mean(axis=(2,3,4)))\n",
    "\n",
    "    # Get the predicted classes\n",
    "    post_act = torch.nn.Softmax(dim=1)\n",
    "    preds = post_act(preds)\n",
    "    pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "    # Map the predicted classes to the label names\n",
    "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "    print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(features_fast_list, axis=0).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the embeddings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in glob.glob(\"/homes/lm004/commercials/annotated_commercials/*.mp4\"):\n",
    "    \n",
    "    features_slow_list = []\n",
    "    features_fast_list = []\n",
    "\n",
    "    for clip in preprocess_video(video_path):\n",
    "        # placeholder for features\n",
    "        features = {}\n",
    "\n",
    "        # Pass the input clip through the model\n",
    "        _ = model(clip)\n",
    "\n",
    "        # Get the features from the last layer\n",
    "        # NB: compute the mean of the features across the last dimensions\n",
    "        features_fast_list.append(features['layer5_fast'].cpu().numpy().mean(axis=(2,3,4)))\n",
    "        features_slow_list.append(features['layer5_slow'].cpu().numpy().mean(axis=(2,3,4)))\n",
    "    \n",
    "    features_fast = np.concatenate(features_fast_list, axis=0)\n",
    "    features_slow = np.concatenate(features_slow_list, axis=0)\n",
    "\n",
    "    stimulus_id = video_path.split('/')[-1].replace('_trimmed.mp4','')\n",
    "\n",
    "    np.save(open(f\"embeddings/{stimulus_id}_fast.npy\", 'wb'), features_fast)\n",
    "    np.save(open(f\"embeddings/{stimulus_id}_slow.npy\", 'wb'), features_slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings_pipeline_dev",
   "language": "python",
   "name": "embeddings_pipeline_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25afc36bdd1f351ba26e01c2272523456c0ab32d7af87ae9c52e485253265920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
