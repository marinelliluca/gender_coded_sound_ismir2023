{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "#import pytorch_lightning as pl\n",
    "from utils import DynamicMultitasker, load_embeddings_and_labels, embedding_dimensions\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import random\n",
    "import yaml, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = \"openl3_env\"\n",
    "voice = True\n",
    "targets_list = [\"Girls/women\", \"Boys/men\"]\n",
    "\n",
    "with open(\"config_save.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "groundtruth_df = pd.read_csv(\"groundtruth_merged.csv\")\n",
    "groundtruth_df.set_index(\"stimulus_id\", inplace=True)\n",
    "\n",
    "\n",
    "emotions_and_mid_level_df = pd.read_csv(\"emotions_and_mid_level.csv\")\n",
    "emotions_and_mid_level_df.set_index(\"stimulus_id\", inplace=True)\n",
    "\n",
    "# drop columns that would introduce noise\n",
    "n_emotions = 7\n",
    "if config[\"drop_non_significant\"]:\n",
    "    to_drop = [\n",
    "        \"Amusing\",  # Extremely low correlations with all the mid-level features\n",
    "        \"Wide/Narrow pitch variation\",  # non significant differences between targets (ANOVA)\n",
    "        \"Repetitive/Non-repetitive\",  # non significant differences between targets (ANOVA)\n",
    "        \"Fast tempo/Slow tempo\",  # non significant differences between targets (ANOVA)\n",
    "    ]\n",
    "    emotions_and_mid_level_df = emotions_and_mid_level_df.drop(columns=to_drop)\n",
    "    n_emotions -= 1  # we dropped Amusing\n",
    "\n",
    "# load quantile normalization parameters\n",
    "with open(f\"quantiles_{which}_voice_{voice}_{len(targets_list)}_cls.json\", \"r\") as f:\n",
    "    quantiles = json.load(f)\n",
    "\n",
    "\n",
    "# scales quantization\n",
    "def value_to_level(value, quantiles):\n",
    "\n",
    "    if value <= quantiles[0]:\n",
    "        return \"low\"\n",
    "    elif value <= quantiles[1]:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters for the model\n",
    "config[\"cls_dict\"][\"target\"] = targets_list # add target list to config\n",
    "params = {\n",
    "    \"input_dim\": embedding_dimensions[\"music\"][which],\n",
    "    \"n_emo\": n_emotions,\n",
    "    \"n_mid\": len(emotions_and_mid_level_df.columns)-n_emotions,\n",
    "    \"cls_dict\": config[\"cls_dict\"],\n",
    "    \"filmed\": False,\n",
    "}\n",
    "\n",
    "# Load model:\n",
    "model = DynamicMultitasker(**params)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "    f\"models/{which}_{voice}_voice_{len(targets_list)}_cls.pt\"\n",
    "    )\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from a random commercial\n",
    "embedding_fn = random.choice(glob.glob(f\"/homes/lm004/all_embeddings/music/{which}/*.npy\"))\n",
    "\n",
    "embedding = np.load(open(embedding_fn, 'rb')).mean(axis=0)\n",
    "\n",
    "youtube_id = embedding_fn.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "print(f\"https://www.youtube.com/watch?v={youtube_id}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_mid_pred, y_emo_pred, y_cls_pred = model(\n",
    "        torch.from_numpy(embedding[np.newaxis,:]).float()\n",
    "    )\n",
    "\n",
    "y_emo_pred = y_emo_pred.numpy()\n",
    "y_mid_pred = y_mid_pred.numpy()\n",
    "y_cls_pred = {k: int(torch.argmax(y_cls_pred[k], dim=1).numpy()) for k in config[\"cls_dict\"]}\n",
    "\n",
    "cls_dict = config[\"cls_dict\"]\n",
    "cls_dict[\"target\"] = targets_list\n",
    "\n",
    "print(\"Predicted target:\", cls_dict[\"target\"][y_cls_pred[\"target\"]])\n",
    "\n",
    "for k in cls_dict:\n",
    "    if k != \"target\":\n",
    "        print(f\"{k}: {cls_dict[k][y_cls_pred[k]]}\")\n",
    "\n",
    "# print level for each emotion\n",
    "for i in range(y_emo_pred.shape[1]):\n",
    "    k = emotions_and_mid_level_df.columns[i]\n",
    "    print(f\"{k}: {value_to_level(y_emo_pred[:,i], quantiles[k])}\")\n",
    "\n",
    "# print level for each mid-level feature\n",
    "for i in range(y_mid_pred.shape[1]):\n",
    "    k = emotions_and_mid_level_df.columns[n_emotions:][i]\n",
    "    print(f\"{k}: {value_to_level(y_mid_pred[:,i], quantiles[k])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings_pipeline_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
